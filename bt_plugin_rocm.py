#!/usr/bin/env python3
"""
Babeltrace2 plugin for reading multiple ROCm profiler SQLite3 databases.

This plugin reads SQLite3 databases generated by rocprofv3 and converts
them into babeltrace2 trace events with each database generating a separate CTF stream.
"""

import bt2
import sqlite3
import os
import json
from typing import Dict, List, Optional, Any, Iterator, Set
from dataclasses import dataclass
from enum import Enum, auto
import re

# Register the plugin
bt2.register_plugin(__name__, "rocm")

class RocmCategory(Enum):
    """ROCm profiler categories from rocprofiler-sdk."""
    HSA_CORE_API = 'HSA_CORE_API'
    HSA_AMD_EXT_API = 'HSA_AMD_EXT_API'
    HSA_IMAGE_EXT_API = 'HSA_IMAGE_EXT_API'
    HSA_FINALIZE_EXT_API = 'HSA_FINALIZE_EXT_API'
    HIP_RUNTIME_API = 'HIP_RUNTIME_API'
    HIP_RUNTIME_API_EXT = 'HIP_RUNTIME_API_EXT'
    HIP_COMPILER_API = 'HIP_COMPILER_API'
    HIP_COMPILER_API_EXT = 'HIP_COMPILER_API_EXT'
    MARKER_CORE_API = 'MARKER_CORE_API'
    MARKER_CONTROL_API = 'MARKER_CONTROL_API'
    MARKER_NAME_API = 'MARKER_NAME_API'
    MEMORY_COPY = 'MEMORY_COPY'
    MEMORY_ALLOCATION = 'MEMORY_ALLOCATION'
    KERNEL_DISPATCH = 'KERNEL_DISPATCH'
    SCRATCH_MEMORY = 'SCRATCH_MEMORY'
    CORRELATION_ID_RETIREMENT = 'CORRELATION_ID_RETIREMENT'
    RCCL_API = 'RCCL_API'
    OMPT = 'OMPT'
    RUNTIME_INITIALIZATION = 'RUNTIME_INITIALIZATION'
    ROCDECODE_API = 'ROCDECODE_API'
    ROCDECODE_API_EXT = 'ROCDECODE_API_EXT'
    ROCJPEG_API = 'ROCJPEG_API'
    HIP_STREAM = 'HIP_STREAM'
    KFD_EVENT_PAGE_MIGRATE = 'KFD_EVENT_PAGE_MIGRATE'
    KFD_EVENT_PAGE_FAULT = 'KFD_EVENT_PAGE_FAULT'
    KFD_EVENT_QUEUE = 'KFD_EVENT_QUEUE'
    KFD_EVENT_UNMAP_FROM_GPU = 'KFD_EVENT_UNMAP_FROM_GPU'
    KFD_EVENT_DROPPED_EVENTS = 'KFD_EVENT_DROPPED_EVENTS'
    KFD_PAGE_MIGRATE = 'KFD_PAGE_MIGRATE'
    KFD_PAGE_FAULT = 'KFD_PAGE_FAULT'
    KFD_QUEUE = 'KFD_QUEUE'

class EventType(Enum):
    """Event type classifications."""
    REGION_START = 'region_start'
    REGION_END = 'region_end'
    KERNEL_DISPATCH_START = 'kernel_dispatch_start'
    KERNEL_DISPATCH_END = 'kernel_dispatch_end'
    MEMORY_COPY_START = 'memory_copy_start'
    MEMORY_COPY_END = 'memory_copy_end'
    MEMORY_ALLOCATION_START = 'memory_allocation_start'
    MEMORY_ALLOCATION_END = 'memory_allocation_end'
    SAMPLE = 'sample'
    COUNTER_COLLECTION = 'counter_collection'

@dataclass
class RocmEventData:
    """Data class for ROCm event information."""
    name: str
    timestamp: int
    duration: Optional[int] = None
    category: Optional[str] = None
    pid: Optional[int] = None
    tid: Optional[int] = None
    agent_id: Optional[int] = None
    queue_id: Optional[int] = None
    stream_id: Optional[int] = None
    channel_id: Optional[str] = None
    event_args: Optional[Dict[str, Any]] = None
    db_index: Optional[int] = None  # Index of the database this event came from

class RocmSourceIterator(bt2._UserMessageIterator):
    """Iterator for the ROCm source component."""

    def __init__(self, config, output_port):
        """Initialize the iterator."""
        self._db_paths = output_port.user_data['db_paths']
        self._trace_class = output_port.user_data['trace_class']
        self._stream_classes = output_port.user_data['stream_classes']
        self._event_classes = output_port.user_data['event_classes']
        self._clock_class = output_port.user_data['clock_class']

        # Create trace and stream instances - create one stream per database
        self._trace = self._trace_class()
        self._connections = []
        self._streams = []
        self._packets = []
        self._stream_states = []

        # Create a stream for each stream class with explicit IDs
        for i, stream_class in enumerate(self._stream_classes):
            # Create stream with explicit ID to ensure separate physical files
            stream = self._trace.create_stream(stream_class, id=i)
            self._streams.append(stream)
            self._packets.append(None)
            self._stream_states.append("stream_beginning")
        # print(f"DEBUG: Created {len(self._streams)} streams for {len(self._db_paths)} databases")

        # Initialize database connections
        for i, db_path in enumerate(self._db_paths):
            # Create connection
            conn = sqlite3.connect(db_path)
            conn.row_factory = sqlite3.Row
            self._connections.append(conn)

        # State management
        self._db_events = []  # Events per database
        self._event_indices = []  # Current event index per database
        self._all_events_loaded = False
        self._current_stream_index = 0  # Which stream we're currently processing
        self._finished_streams = set()  # Track which streams are done

        # Load all events from all databases
        self._load_all_events()

    def _load_all_events(self):
        """Load events from all databases, assign all events from each DB to its own stream (file)."""
        self._db_events = []
        self._event_indices = []
        total_events = 0
        global_min = None
        global_max = None
        for db_index, db_path in enumerate(self._db_paths):
            try:
                print(f"Loading events from database {db_index + 1}: {db_path}")
                conn = self._connections[db_index]
                events = self._load_events_from_db(conn, db_index)
                if events:
                    events.sort(key=lambda event: event.timestamp)
                    self._db_events.append(events)
                    self._event_indices.append(0)
                    total_events += len(events)
                    db_min = events[0].timestamp
                    db_max = events[-1].timestamp
                    print(f"Stream {db_index + 1} event count: {len(events)}, timestamp range: {db_min} to {db_max}")
                    if global_min is None or db_min < global_min:
                        global_min = db_min
                    if global_max is None or db_max > global_max:
                        global_max = db_max
                else:
                    self._db_events.append([])
                    self._event_indices.append(0)
                    print(f"Stream {db_index + 1} event count: 0")
            except Exception as e:
                print(f"Error loading events from database {db_index + 1} ({db_path}): {e}")
                self._db_events.append([])
                self._event_indices.append(0)
        self._all_events_loaded = True
        print(f"Total events loaded from {len(self._db_paths)} databases: {total_events}")
        print(f"Global timestamp range after sorting: {global_min} to {global_max}")
        print(f"Events distributed across {len(self._db_events)} streams (files):")

    def _load_events_from_db(self, conn, db_index):
        """Load events from a specific database."""
        events = []

        # Try loading from views first (if they exist)
        if self._table_exists('regions', conn):
            events.extend(self._load_region_events_from_view(conn, db_index))

        if self._table_exists('kernels', conn):
            events.extend(self._load_kernel_events_from_view(conn, db_index))

        if self._table_exists('memory_copies', conn):
            events.extend(self._load_memory_copy_events_from_view(conn, db_index))

        if self._table_exists('samples', conn):
            events.extend(self._load_sample_events_from_view(conn, db_index))

        return events

    def _table_exists(self, table_name: str, conn) -> bool:
        """Check if a table or view exists in the database."""
        try:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT name FROM sqlite_master WHERE (type='table' OR type='view') AND name=?",
                (table_name,)
            )
            return cursor.fetchone() is not None
        except sqlite3.Error:
            return False

    def _load_region_events_from_view(self, conn, db_index):
        """Load region events from the regions view with full information."""
        events = []
        try:
            cursor = conn.cursor()

            query = """
            SELECT
                id, guid, category, name, nid, pid, tid, start, end, duration,
                event_id, stack_id, parent_stack_id, corr_id, extdata,
                call_stack, line_info
            FROM regions
            ORDER BY start
            """
            cursor.execute(query)

            for row in cursor.fetchall():
                category = row[2].lower() if row[2] else 'unknown'
                duration = row[9] if row[9] is not None else (row[8] - row[7] if row[8] and row[7] else 0)

                # Create comprehensive event args
                common_args = {
                    'region_id': row[0] or 0,
                    'guid': row[1] or '',
                    'name': row[3] or '',
                    'category': row[2] or '',
                    'nid': row[4] or 0,
                    'pid': row[5] or 0,
                    'tid': row[6] or 0,
                    'event_id': row[10] or 0,
                    'stack_id': row[11] or 0,
                    'parent_stack_id': row[12] or 0,
                    'correlation_id': row[13] or 0,
                    'extdata': str(row[14]) if row[14] else '{}',
                    'call_stack': str(row[15]) if row[15] else '{}',
                    'line_info': str(row[16]) if row[16] else '{}'
                }

                # Region start event
                start_args = common_args.copy()
                start_args.update({
                    'event_type': EventType.REGION_START.value,
                    'duration': 0
                })

                events.append(RocmEventData(
                    name=f"{row[3]}_start",
                    timestamp=row[7],
                    category=category,
                    pid=row[5],
                    tid=row[6],
                    event_args=start_args,
                    db_index=db_index
                ))

                # Region end event
                end_args = common_args.copy()
                end_args.update({
                    'event_type': EventType.REGION_END.value,
                    'duration': duration
                })

                events.append(RocmEventData(
                    name=f"{row[3]}_end",
                    timestamp=row[8],
                    category=category,
                    pid=row[5],
                    tid=row[6],
                    event_args=end_args,
                    db_index=db_index
                ))

        except sqlite3.Error as e:
            print(f"Error loading region events from view: {e}")

        return events

    def _load_kernel_events_from_view(self, conn, db_index):
        """Load kernel events from the kernels view with full information."""
        events = []
        try:
            cursor = conn.cursor()

            query = """
            SELECT
                id, guid, tid, category, region, name, nid, pid,
                agent_abs_index, agent_log_index, agent_type_index, agent_type,
                code_object_id, kernel_id, dispatch_id, stream_id, queue_id,
                queue, stream, start, end, duration,
                grid_x, grid_y, grid_z, workgroup_x, workgroup_y, workgroup_z,
                lds_size, scratch_size, static_lds_size, static_scratch_size,
                stack_id, parent_stack_id, corr_id
            FROM kernels
            ORDER BY start
            """
            cursor.execute(query)

            for row in cursor.fetchall():
                kernel_name = row[5] or f"kernel_{row[0]}"
                category = row[3].lower() if row[3] else 'kernel_dispatch'
                duration = row[21] if row[21] is not None else (row[20] - row[19] if row[20] and row[19] else 0)

                # Create comprehensive kernel args
                common_args = {
                    'kernel_id': row[0] or 0,
                    'guid': row[1] or '',
                    'tid': row[2] or 0,
                    'category': row[3] or '',
                    'region': row[4] or '',
                    'name': kernel_name,
                    'nid': row[6] or 0,
                    'pid': row[7] or 0,
                    'agent_abs_index': row[8] or 0,
                    'agent_log_index': row[9] or 0,
                    'agent_type_index': row[10] or 0,
                    'agent_type': row[11] or '',
                    'code_object_id': row[12] or 0,
                    'kernel_symbol_id': row[13] or 0,
                    'dispatch_id': row[14] or 0,
                    'stream_id': row[15] or 0,
                    'queue_id': row[16] or 0,
                    'queue_name': row[17] or '',
                    'stream_name': row[18] or '',
                    'grid_size_x': row[22] or 0,
                    'grid_size_y': row[23] or 0,
                    'grid_size_z': row[24] or 0,
                    'workgroup_size_x': row[25] or 0,
                    'workgroup_size_y': row[26] or 0,
                    'workgroup_size_z': row[27] or 0,
                    'lds_size': row[28] or 0,
                    'scratch_size': row[29] or 0,
                    'static_lds_size': row[30] or 0,
                    'static_scratch_size': row[31] or 0,
                    'stack_id': row[32] or 0,
                    'parent_stack_id': row[33] or 0,
                    'correlation_id': row[34] or 0,
                    'duration': row[21] if row[21] is not None else (row[20] - row[19] if row[20] and row[19] else 0)
                }

                # Kernel start event
                start_args = common_args.copy()
                start_args.update({
                    'event_type': EventType.KERNEL_DISPATCH_START.value,
                    'duration': 0
                })

                events.append(RocmEventData(
                    name=f"kernel_dispatch_start",
                    timestamp=row[19],
                    category=category,
                    pid=row[7],
                    tid=row[2],
                    event_args=start_args,
                    db_index=db_index
                ))

                # Kernel end event
                end_args = common_args.copy()
                end_args.update({
                    'event_type': EventType.KERNEL_DISPATCH_END.value,
                    'duration': duration
                })

                events.append(RocmEventData(
                    name=f"kernel_dispatch_end",
                    timestamp=row[20],
                    category=category,
                    pid=row[7],
                    tid=row[2],
                    event_args=end_args,
                    db_index=db_index
                ))

        except sqlite3.Error as e:
            print(f"Error loading kernel events from view: {e}")

        return events

    def _load_memory_copy_events_from_view(self, conn, db_index):
        """Load memory copy events from the memory_copies view with full information."""
        events = []
        try:
            cursor = conn.cursor()

            query = """
            SELECT
                id, guid, category, nid, pid, tid, start, end, duration,
                name, region_name, stream_id, queue_id, stream_name, queue_name,
                size, dst_device, dst_agent_abs_index, dst_agent_log_index,
                dst_agent_type_index, dst_agent_type, dst_address,
                src_device, src_agent_abs_index, src_agent_log_index,
                src_agent_type_index, src_agent_type, src_address,
                stack_id, parent_stack_id, corr_id
            FROM memory_copies
            ORDER BY start
            """
            cursor.execute(query)

            for row in cursor.fetchall():
                copy_name = row[9] or f"memory_copy_{row[0]}"
                category = row[2].lower() if row[2] else 'memory_copy'
                duration = row[8] if row[8] is not None else (row[7] - row[6] if row[7] and row[6] else 0)

                # Create comprehensive memory copy args
                common_args = {
                    'copy_id': row[0] or 0,
                    'guid': row[1] or '',
                    'category': row[2] or '',
                    'nid': row[3] or 0,
                    'pid': row[4] or 0,
                    'tid': row[5] or 0,
                    'name': copy_name,
                    'region_name': row[10] or '',
                    'stream_id': row[11] or 0,
                    'queue_id': row[12] or 0,
                    'stream_name': row[13] or '',
                    'queue_name': row[14] or '',
                    'size': row[15] or 0,
                    'dst_device': row[16] or '',
                    'dst_agent_abs_index': row[17] or 0,
                    'dst_agent_log_index': row[18] or 0,
                    'dst_agent_type_index': row[19] or 0,
                    'dst_agent_type': row[20] or '',
                    'dst_address': row[21] or 0,
                    'src_device': row[22] or '',
                    'src_agent_abs_index': row[23] or 0,
                    'src_agent_log_index': row[24] or 0,
                    'src_agent_type_index': row[25] or 0,
                    'src_agent_type': row[26] or '',
                    'src_address': row[27] or 0,
                    'stack_id': row[28] or 0,
                    'parent_stack_id': row[29] or 0,
                    'correlation_id': row[30] or 0
                }

                # Memory copy start event
                start_args = common_args.copy()
                start_args.update({
                    'event_type': EventType.MEMORY_COPY_START.value,
                    'duration': 0
                })

                events.append(RocmEventData(
                    name=f"memory_copy_start",
                    timestamp=row[6],
                    category=category,
                    pid=row[4],
                    tid=row[5],
                    event_args=start_args,
                    db_index=db_index
                ))

                # Memory copy end event
                end_args = common_args.copy()
                end_args.update({
                    'event_type': EventType.MEMORY_COPY_END.value,
                    'duration': duration
                })

                events.append(RocmEventData(
                    name=f"memory_copy_end",
                    timestamp=row[7],
                    category=category,
                    pid=row[4],
                    tid=row[5],
                    event_args=end_args,
                    db_index=db_index
                ))

        except sqlite3.Error as e:
            print(f"Error loading memory copy events from view: {e}")

        return events

    def _load_sample_events_from_view(self, conn, db_index):
        """Load sample events from the samples view with full information."""
        events = []
        try:
            cursor = conn.cursor()

            query = """
            SELECT
                id, guid, category, name, nid, pid, tid, timestamp,
                event_id, stack_id, parent_stack_id, corr_id,
                extdata, call_stack, line_info
            FROM samples
            ORDER BY timestamp
            """
            cursor.execute(query)

            for row in cursor.fetchall():
                sample_name = row[3] or f"sample_{row[0]}"

                # Create comprehensive sample args
                sample_args = {
                    'sample_id': row[0] or 0,
                    'guid': row[1] or '',
                    'category': row[2] or '',
                    'sample_name': sample_name,
                    'nid': row[4] or 0,
                    'pid': row[5] or 0,
                    'tid': row[6] or 0,
                    'event_id': row[8] or 0,
                    'stack_id': row[9] or 0,
                    'parent_stack_id': row[10] or 0,
                    'correlation_id': row[11] or 0,
                    'extdata': str(row[12]) if row[12] else '{}',
                    'call_stack': str(row[13]) if row[13] else '{}',
                    'line_info': str(row[14]) if row[14] else '{}',
                    'event_type': EventType.SAMPLE.value
                }

                events.append(RocmEventData(
                    name=f"sample",
                    timestamp=row[7],
                    category='sample',
                    pid=row[5],
                    tid=row[6],
                    event_args=sample_args,
                    db_index=db_index
                ))

        except sqlite3.Error as e:
            print(f"Error loading sample events from view: {e}")

        return events

    def __next__(self):
        """Return the next message, ensuring strict global timestamp order across all streams."""
        # For each stream, determine the next message type and its effective timestamp
        candidates = []
        for stream_idx in range(len(self._streams)):
            if stream_idx in self._finished_streams:
                continue

            stream_state = self._stream_states[stream_idx]
            db_events = self._db_events[stream_idx]
            event_index = self._event_indices[stream_idx]

            # Determine the next message type and its effective timestamp
            if stream_state == "stream_beginning":
                # Use the timestamp of the first event, or 0 if no events
                ts = db_events[0].timestamp if db_events else 0
                candidates.append((ts, stream_idx, "stream_beginning"))
            elif stream_state == "packet_beginning":
                ts = db_events[0].timestamp if db_events else 0
                candidates.append((ts, stream_idx, "packet_beginning"))
            elif stream_state == "events" and event_index < len(db_events):
                ts = db_events[event_index].timestamp
                candidates.append((ts, stream_idx, "event"))
            elif stream_state == "events" and event_index >= len(db_events):
                # All events done, packet end
                ts = db_events[-1].timestamp if db_events else 0
                candidates.append((ts, stream_idx, "packet_end"))
            elif stream_state == "packet_end":
                # Use a timestamp just after the last event
                ts = (db_events[-1].timestamp + 1) if db_events else 1
                candidates.append((ts, stream_idx, "stream_end"))
            # stream_end: do nothing (already finished)

        if not candidates:
            raise StopIteration

        # Select the candidate with the lowest timestamp (and lowest stream_idx for tie-break)
        candidates.sort()
        ts, stream_idx, msg_type = candidates[0]

        stream_state = self._stream_states[stream_idx]
        db_events = self._db_events[stream_idx]
        event_index = self._event_indices[stream_idx]

        if msg_type == "stream_beginning":
            self._stream_states[stream_idx] = "packet_beginning"
            return self._create_stream_beginning_message(self._streams[stream_idx])

        if msg_type == "packet_beginning":
            if self._packets[stream_idx] is None:
                self._packets[stream_idx] = self._streams[stream_idx].create_packet()
            self._stream_states[stream_idx] = "events"
            first_timestamp = db_events[0].timestamp if db_events else 0
            return self._create_packet_beginning_message(self._packets[stream_idx], default_clock_snapshot=first_timestamp)

        if msg_type == "event":
            event_data = db_events[event_index]
            self._event_indices[stream_idx] += 1

            packet = self._packets[stream_idx]
            event_class_name = self._get_event_class_name_by_category(event_data)
            prefixed_event_class_name = f"{stream_idx}_{event_class_name}"
            event_class = self._event_classes.get(prefixed_event_class_name)

            if event_class is None:
                # Fall back to generic event if specific class not found
                if event_class_name.endswith('_start'):
                    prefixed_event_class_name = f"{stream_idx}_generic_event_start"
                elif event_class_name.endswith('_end'):
                    prefixed_event_class_name = f"{stream_idx}_generic_event_end"
                else:
                    prefixed_event_class_name = f"{stream_idx}_generic_event"
                event_class = self._event_classes.get(prefixed_event_class_name)

            if event_class is None:
                raise ValueError(f"No event class found for '{prefixed_event_class_name}' (stream {stream_idx})")

            msg = self._create_event_message(
                event_class,
                packet,
                default_clock_snapshot=event_data.timestamp
            )
            self._set_event_fields(msg, event_data, event_class_name)
            return msg

        if msg_type == "packet_end":
            self._stream_states[stream_idx] = "packet_end"
            last_timestamp = db_events[-1].timestamp if db_events else 0
            return self._create_packet_end_message(self._packets[stream_idx], default_clock_snapshot=last_timestamp)

        if msg_type == "stream_end":
            self._stream_states[stream_idx] = "stream_end"
            self._finished_streams.add(stream_idx)
            return self._create_stream_end_message(self._streams[stream_idx])

        raise StopIteration

    def _get_event_class_name_by_category(self, event_data: RocmEventData) -> str:
        """Get the event class name based on event category and name."""
        # Determine if this is a start or end event based on event_type
        suffix = ''
        if hasattr(event_data, 'event_args') and event_data.event_args:
            event_type = event_data.event_args.get('event_type', '')
            if event_type == 'region_start':
                suffix = '_start'
            elif event_type == 'region_end':
                suffix = '_end'
            elif event_type.endswith('_start'):
                suffix = '_start'
            elif event_type.endswith('_end'):
                suffix = '_end'

        # Also check event name for start/end patterns if event_type doesn't indicate
        if not suffix and hasattr(event_data, 'name') and event_data.name:
            if event_data.name.endswith('_start'):
                suffix = '_start'
            elif event_data.name.endswith('_end'):
                suffix = '_end'

        if hasattr(event_data, 'category') and event_data.category:
            category = event_data.category.lower()

            # Map categories to specific event types
            if category == 'hip_runtime_api_ext' or category == 'hip_runtime_api':
                return f'hip_runtime_region_event{suffix}'
            elif category == 'hip_compiler_api_ext' or category == 'hip_compiler_api':
                return f'hip_compiler_region_event{suffix}'
            elif category == 'hsa_core_api':
                return f'hsa_core_region_event{suffix}'
            elif category == 'hsa_amd_ext_api':
                return f'hsa_amd_ext_region_event{suffix}'
            elif category == 'marker_core_api':
                return f'marker_core_region_event{suffix}'
            elif category == 'kernel_dispatch':
                return f'kernel_dispatch_event{suffix}'
            elif category == 'memory_copy':
                return f'memory_copy_event{suffix}'
            elif category == 'memory_allocation':
                return f'memory_allocation_event{suffix}'
            elif category == 'counter_collection':
                return f'counter_collection_event{suffix}'
            elif category == 'sample':
                return f'sample_event{suffix}'
            elif "region" in event_data.name:
                return f'region_event{suffix}'

        # Fall back to original logic
        return self._get_event_class_name(event_data.name)

    def _get_event_class_name(self, event_name: str) -> str:
        """Get the event class name based on event name (fallback method)."""
        # Determine if this is a start or end event
        suffix = ''
        if event_name.endswith('_start'):
            suffix = '_start'
        elif event_name.endswith('_end'):
            suffix = '_end'

        # Simple mapping based on event name patterns
        if 'kernel_dispatch' in event_name.lower():
            return f'kernel_dispatch_event{suffix}'
        elif 'kernel' in event_name.lower():
            return f'kernel_dispatch_event{suffix}'
        elif 'memory_allocation' in event_name.lower():
            return f'memory_allocation_event{suffix}'
        elif 'memory_copy' in event_name.lower():
            return f'memory_copy_event{suffix}'
        elif 'memory' in event_name.lower():
            return f'memory_copy_event{suffix}'
        elif 'sample' in event_name.lower():
            return f'sample_event{suffix}'
        elif 'region' in event_name.lower():
            return f'region_event{suffix}'
        else:
            # Default fallback
            return f'generic_event{suffix}'

    def _set_event_fields(self, msg, event_data, event_class_name):
        """Set event fields based on event data and event class type."""
        payload = msg.event.payload_field

        # Set common fields that all events have
        if hasattr(event_data, 'name') and event_data.name is not None:
            if 'name' in payload:
                payload['name'] = event_data.name

        if hasattr(event_data, 'category') and event_data.category is not None and 'category' in payload:
            payload['category'] = event_data.category

        if hasattr(event_data, 'duration') and event_data.duration is not None and 'duration' in payload:
            payload['duration'] = event_data.duration

        # For all event types, try to set any matching fields from event_args
        if hasattr(event_data, 'event_args') and event_data.event_args:
            for key, value in event_data.event_args.items():
                # Skip event_type since it's now encoded in the class name
                if key == 'event_type':
                    continue
                if key in payload and value is not None:
                    payload[key] = value

    def __del__(self):
        """Cleanup database connections."""
        if hasattr(self, '_connections'):
            for conn in self._connections:
                if conn:
                    conn.close()

@bt2.plugin_component_class
class RocmSource(bt2._UserSourceComponent, message_iterator_class=RocmSourceIterator):
    """Source component for reading multiple ROCm profiler SQLite3 databases."""

    def __init__(self, config, params, obj):
        """Initialize the source component."""
        # Get database paths from parameters - support both single and multiple databases
        db_path_param = params.get('db-path', None) or params.get('db_path', None)
        db_paths_param = params.get('db-paths', None) or params.get('db_paths', None)

        if db_paths_param:
            # Multiple databases specified
            if isinstance(db_paths_param, (list, tuple)) or (hasattr(db_paths_param, '__iter__') and not isinstance(db_paths_param, str) and not hasattr(db_paths_param, '__str__')):
                # Handle bt2 ArrayValue or regular Python list/tuple
                try:
                    # Try to convert to list of strings
                    self._db_paths = [str(path) for path in db_paths_param]
                except (TypeError, ValueError):
                    # If it's not iterable, treat as single string
                    self._db_paths = [str(db_paths_param)]
            else:
                # Handle string formats (including bt2 string values)
                db_paths_str = str(db_paths_param).strip()

                # Check if it contains shell-style brace expansion patterns
                if '{' in db_paths_str and '}' in db_paths_str:
                    # This might be a shell pattern that wasn't expanded
                    # Try to expand it using subprocess to invoke the shell
                    import subprocess
                    try:
                        # Use shell to expand the pattern
                        result = subprocess.run(['bash', '-c', f'echo {db_paths_str}'],
                                              capture_output=True, text=True, check=True)
                        expanded_str = result.stdout.strip()
                        if expanded_str and expanded_str != db_paths_str:
                            # Successfully expanded, split by spaces
                            self._db_paths = expanded_str.split()
                            print(f"Expanded pattern '{db_paths_str}' to {len(self._db_paths)} files: {self._db_paths}")
                        else:
                            # Pattern didn't expand, treat as literal string
                            self._db_paths = [db_paths_str]
                    except subprocess.CalledProcessError as e:
                        # If shell expansion fails, treat as literal string
                        print(f"Shell expansion failed for '{db_paths_str}': {e}, treating as literal filename")
                        self._db_paths = [db_paths_str]
                else:
                    # Regular comma-separated paths or single path
                    if ',' in db_paths_str:
                        self._db_paths = [path.strip() for path in db_paths_str.split(',')]
                    else:
                        self._db_paths = [db_paths_str]
        elif db_path_param:
            # Single database path (backwards compatibility)
            self._db_paths = [str(db_path_param)]
        else:
            # Default fallback
            self._db_paths = ['24228_results.db']

        if not self._db_paths:
            raise ValueError("Database path parameter 'db-path' or 'db-paths' is required")        # Resolve and validate all database files exist
        resolved_paths = []
        missing_files = []
        for db_path in self._db_paths:
            # Convert to absolute path if relative
            if not os.path.isabs(db_path):
                # Try relative to current working directory first
                abs_path = os.path.abspath(db_path)
            else:
                abs_path = db_path

            if os.path.exists(abs_path):
                resolved_paths.append(abs_path)
            else:
                missing_files.append(db_path)

        # Update the db_paths with resolved paths
        self._db_paths = resolved_paths

        if missing_files:
            if len(missing_files) == 1:
                raise FileNotFoundError(f"Database file not found: {missing_files[0]}")
            else:
                raise FileNotFoundError(f"Database files not found: {', '.join(missing_files)}")

        print(f"Initializing ROCm source with {len(self._db_paths)} databases:")
        for i, db_path in enumerate(self._db_paths):
            print(f"  Database {i+1}: {db_path}")

        # Create trace class with manual ID assignment
        self._trace_class = super()._create_trace_class(assigns_automatic_stream_class_id=False)

        # Create clock class
        self._clock_class = self._create_clock_class(
            name="rocm_clock",
            description="ROCm profiler clock",
            frequency=1_000_000_000  # Nanoseconds
        )

        # Always create one stream per database
        num_streams_needed = len(self._db_paths)
        print(f"Creating {num_streams_needed} streams (one per database)")

        self._stream_classes = []
        for i in range(num_streams_needed):
            stream_class = self._trace_class.create_stream_class(
                id=i,
                name=f"rocm_stream_{i}",
                default_clock_class=self._clock_class,
                supports_packets=True,
                packets_have_beginning_default_clock_snapshot=True,
                packets_have_end_default_clock_snapshot=True,
                assigns_automatic_stream_id=False
            )
            self._stream_classes.append(stream_class)
        print(f"Created {len(self._stream_classes)} stream classes (one per database)")

        # Create event classes
        self._event_classes = self._create_event_classes()

        # Create output port
        self._add_output_port("out", {
            'db_paths': self._db_paths,
            'trace_class': self._trace_class,
            'stream_classes': self._stream_classes,
            'event_classes': self._event_classes,
            'clock_class': self._clock_class
        })

    def _create_clock_class(self, name: str, description: str, frequency: int):
        """Create a clock class for ROCm events."""
        clock_class = super()._create_clock_class(
            name=name,
            description=description,
            frequency=frequency
        )
        return clock_class

    def _create_event_classes(self) -> Dict[str, Any]:
        """Create event classes for different ROCm event types."""
        if not self._stream_classes:
            print("ERROR: No stream classes available for event class creation")
            return {}

        print(f"Creating event classes for {len(self._stream_classes)} stream classes")
        event_classes = {}

        # Create event classes for each stream class
        for stream_idx, stream_class in enumerate(self._stream_classes):
            if stream_class is None:
                print(f"ERROR: Stream class {stream_idx} is None")
                continue

            # Helper function to create both start and end event classes for this stream
            def create_event_class_pair(base_name: str, field_class_factory, stream_index=stream_idx):
                """Create both start and end event classes for a given base event type."""
                # Create start event class
                start_name = f"{base_name}_start"
                start_field_class = field_class_factory()
                start_event_class = stream_class.create_event_class(
                    name=start_name,
                    payload_field_class=start_field_class
                )
                event_classes[f"{stream_index}_{start_name}"] = start_event_class

                # Create end event class
                end_name = f"{base_name}_end"
                end_field_class = field_class_factory()
                end_event_class = stream_class.create_event_class(
                    name=end_name,
                    payload_field_class=end_field_class
                )
                event_classes[f"{stream_index}_{end_name}"] = end_event_class

                # Also create the base event class for compatibility
                base_field_class = field_class_factory()
                base_event_class = stream_class.create_event_class(
                    name=base_name,
                    payload_field_class=base_field_class
                )
                event_classes[f"{stream_index}_{base_name}"] = base_event_class

            # Create field class factory for region events
            def create_region_field_class():
                fc = self._trace_class.create_structure_field_class()
                fc.append_member("region_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("guid", self._trace_class.create_string_field_class())
                fc.append_member("name", self._trace_class.create_string_field_class())
                fc.append_member("category", self._trace_class.create_string_field_class())
                fc.append_member("nid", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("pid", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("tid", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("event_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("stack_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("parent_stack_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("correlation_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("duration", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("extdata", self._trace_class.create_string_field_class())
                fc.append_member("call_stack", self._trace_class.create_string_field_class())
                fc.append_member("line_info", self._trace_class.create_string_field_class())
                return fc

            # Create field class factory for kernel dispatch events
            def create_kernel_dispatch_field_class():
                fc = self._trace_class.create_structure_field_class()
                fc.append_member("kernel_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("guid", self._trace_class.create_string_field_class())
                fc.append_member("tid", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("category", self._trace_class.create_string_field_class())
                fc.append_member("region", self._trace_class.create_string_field_class())
                fc.append_member("name", self._trace_class.create_string_field_class())
                fc.append_member("nid", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("pid", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("agent_abs_index", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("agent_log_index", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("agent_type_index", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("agent_type", self._trace_class.create_string_field_class())
                fc.append_member("code_object_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("dispatch_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("stream_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("queue_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("queue_name", self._trace_class.create_string_field_class())
                fc.append_member("stream_name", self._trace_class.create_string_field_class())
                fc.append_member("grid_size_x", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("grid_size_y", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("grid_size_z", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("workgroup_size_x", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("workgroup_size_y", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("workgroup_size_z", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("lds_size", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("scratch_size", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("static_lds_size", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("static_scratch_size", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("stack_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("parent_stack_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("correlation_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("duration", self._trace_class.create_signed_integer_field_class(64))
                return fc

            # Create field class factory for memory copy events
            def create_memory_copy_field_class():
                fc = self._trace_class.create_structure_field_class()
                fc.append_member("copy_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("guid", self._trace_class.create_string_field_class())
                fc.append_member("category", self._trace_class.create_string_field_class())
                fc.append_member("nid", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("pid", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("tid", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("name", self._trace_class.create_string_field_class())
                fc.append_member("region_name", self._trace_class.create_string_field_class())
                fc.append_member("stream_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("queue_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("stream_name", self._trace_class.create_string_field_class())
                fc.append_member("queue_name", self._trace_class.create_string_field_class())
                fc.append_member("size", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("dst_device", self._trace_class.create_string_field_class())
                fc.append_member("dst_agent_abs_index", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("dst_agent_log_index", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("dst_agent_type_index", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("dst_agent_type", self._trace_class.create_string_field_class())
                fc.append_member("dst_address", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("src_device", self._trace_class.create_string_field_class())
                fc.append_member("src_agent_abs_index", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("src_agent_log_index", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("src_agent_type_index", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("src_agent_type", self._trace_class.create_string_field_class())
                fc.append_member("src_address", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("stack_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("parent_stack_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("correlation_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("duration", self._trace_class.create_signed_integer_field_class(64))
                return fc

            # Create field class factory for memory allocation events
            def create_memory_allocation_field_class():
                fc = self._trace_class.create_structure_field_class()
                fc.append_member("alloc_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("guid", self._trace_class.create_string_field_class())
                fc.append_member("category", self._trace_class.create_string_field_class())
                fc.append_member("name", self._trace_class.create_string_field_class())
                fc.append_member("nid", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("pid", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("tid", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("size", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("address", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("duration", self._trace_class.create_signed_integer_field_class(64))
                return fc

            # Create field class factory for counter collection events
            def create_counter_collection_field_class():
                fc = self._trace_class.create_structure_field_class()
                fc.append_member("counter_id", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("name", self._trace_class.create_string_field_class())
                fc.append_member("category", self._trace_class.create_string_field_class())
                fc.append_member("value", self._trace_class.create_signed_integer_field_class(64))
                fc.append_member("duration", self._trace_class.create_signed_integer_field_class(64))
                return fc

            # Create field class factory for generic events
            def create_generic_field_class():
                fc = self._trace_class.create_structure_field_class()
                fc.append_member("name", self._trace_class.create_string_field_class())
                fc.append_member("category", self._trace_class.create_string_field_class())
                fc.append_member("duration", self._trace_class.create_signed_integer_field_class(64))
                return fc

            # Create event class pairs for all event types
            create_event_class_pair("region_event", create_region_field_class)
            create_event_class_pair("kernel_dispatch_event", create_kernel_dispatch_field_class)
            create_event_class_pair("memory_copy_event", create_memory_copy_field_class)
            create_event_class_pair("memory_allocation_event", create_memory_allocation_field_class)
            create_event_class_pair("counter_collection_event", create_counter_collection_field_class)
            create_event_class_pair("generic_event", create_generic_field_class)

            # Create event class pairs for all region event types
            create_event_class_pair("hip_runtime_region_event", create_region_field_class)
            create_event_class_pair("hip_compiler_region_event", create_region_field_class)
            create_event_class_pair("hsa_core_region_event", create_region_field_class)
            create_event_class_pair("hsa_amd_ext_region_event", create_region_field_class)
            create_event_class_pair("marker_core_region_event", create_region_field_class)

            # Sample event class (single event, no start/end)
            sample_payload_fc = self._trace_class.create_structure_field_class()
            sample_payload_fc.append_member("sample_id", self._trace_class.create_signed_integer_field_class(64))
            sample_payload_fc.append_member("guid", self._trace_class.create_string_field_class())
            sample_payload_fc.append_member("category", self._trace_class.create_string_field_class())
            sample_payload_fc.append_member("name", self._trace_class.create_string_field_class())
            sample_payload_fc.append_member("nid", self._trace_class.create_signed_integer_field_class(64))
            sample_payload_fc.append_member("pid", self._trace_class.create_signed_integer_field_class(64))
            sample_payload_fc.append_member("tid", self._trace_class.create_signed_integer_field_class(64))
            sample_payload_fc.append_member("event_id", self._trace_class.create_signed_integer_field_class(64))
            sample_payload_fc.append_member("stack_id", self._trace_class.create_signed_integer_field_class(64))
            sample_payload_fc.append_member("parent_stack_id", self._trace_class.create_signed_integer_field_class(64))
            sample_payload_fc.append_member("correlation_id", self._trace_class.create_signed_integer_field_class(64))
            sample_payload_fc.append_member("extdata", self._trace_class.create_string_field_class())
            sample_payload_fc.append_member("call_stack", self._trace_class.create_string_field_class())
            sample_payload_fc.append_member("line_info", self._trace_class.create_string_field_class())
            sample_event_class = stream_class.create_event_class(
                name="sample_event",
                payload_field_class=sample_payload_fc
            )
            event_classes[f"{stream_idx}_sample_event"] = sample_event_class

        print(f"Created {len(event_classes)} event classes for {len(self._stream_classes)} streams")
        return event_classes

    @staticmethod
    def _user_query(priv_executor, obj, query, params):
        """Handle query requests."""
        # print(f"DEBUG: _user_query called with query={query}, params={params}")

        if query == "babeltrace.support-info":
            # print(f"DEBUG: Handling babeltrace.support-info query")
            # Support SQLite3 files that contain ROCm profiler tables
            try:
                input_value = params.get('input')
                # print(f"DEBUG: input_value={input_value}")

                if not input_value:
                    # print("DEBUG: No input value, returning 0.0")
                    return bt2.RealValue(0.0)

                # Check if it's a file path
                if not isinstance(input_value, str):
                    # print(f"DEBUG: input_value is not string, type={type(input_value)}")
                    return bt2.RealValue(0.0)

                # Check if file exists and is SQLite
                if not os.path.exists(input_value):
                    # print(f"DEBUG: File does not exist: {input_value}")
                    return bt2.RealValue(0.0)

                # Quick check if it's a SQLite database with ROCm tables
                conn = sqlite3.connect(input_value)
                cursor = conn.cursor()

                # Check for ROCm-specific tables
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name LIKE 'rocpd_%'")
                rocm_tables = cursor.fetchall()
                # print(f"DEBUG: Found ROCm tables: {rocm_tables}")

                conn.close()

                if rocm_tables:
                    # print("DEBUG: Found ROCm tables, returning 1.0")
                    return bt2.RealValue(1.0)  # Perfect match
                else:
                    # print("DEBUG: No ROCm tables found, returning 0.0")
                    return bt2.RealValue(0.0)  # Not a ROCm database

            except Exception as e:
                # print(f"DEBUG: Exception in support-info query: {e}")
                return bt2.RealValue(0.0)

        elif query == "babeltrace.mip-version":
            # print(f"DEBUG: Handling babeltrace.mip-version query")
            # Declare MIP version support
            return bt2.SignedIntegerValue(0)  # Support MIP version 0

        # print(f"DEBUG: Unknown query: {query}")
        return None

    @classmethod
    def _user_get_supported_mip_versions(cls, params, obj, log_level):
        """Return the supported MIP versions."""
        return [0]

    def _estimate_total_events(self):
        """Estimate total number of events across all databases."""
        total_events = 0

        for db_index, db_path in enumerate(self._db_paths):
            try:
                # Open temporary connection for estimation
                conn = sqlite3.connect(db_path)
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()

                # Count events from each table/view
                tables_to_count = ['regions', 'kernels', 'memory_copies', 'samples']
                db_events = 0

                for table in tables_to_count:
                    if self._table_exists_temp(table, conn):
                        if table == 'regions':
                            # Regions create start and end events
                            cursor.execute(f"SELECT COUNT(*) FROM {table}")
                            count = cursor.fetchone()[0]
                            db_events += count * 2  # start and end events
                        elif table == 'kernels':
                            # Kernels create start and end events
                            cursor.execute(f"SELECT COUNT(*) FROM {table}")
                            count = cursor.fetchone()[0]
                            db_events += count * 2  # start and end events
                        elif table == 'memory_copies':
                            # Memory copies create start and end events
                            cursor.execute(f"SELECT COUNT(*) FROM {table}")
                            count = cursor.fetchone()[0]
                            db_events += count * 2  # start and end events
                        elif table == 'samples':
                            # Samples create single events
                            cursor.execute(f"SELECT COUNT(*) FROM {table}")
                            count = cursor.fetchone()[0]
                            db_events += count

                total_events += db_events
                print(f"Database {db_index + 1} ({db_path}): estimated {db_events} events")

                # Close temporary connection
                conn.close()

            except Exception as e:
                print(f"Error estimating events for database {db_index + 1} ({db_path}): {e}")

        return total_events

    def _table_exists_temp(self, table_name: str, conn) -> bool:
        """Check if a table or view exists in the database (for temporary connections)."""
        try:
            cursor = conn.cursor()
            cursor.execute(
                "SELECT name FROM sqlite_master WHERE (type='table' OR type='view') AND name=?",
                (table_name,)
            )
            return cursor.fetchone() is not None
        except sqlite3.Error:
            return False
